{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Phisihing Detection Using Advanced NLP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit all the Mardown cells below with the appropriate information\n",
    "# Run all cells, containing your code\n",
    "# Save this Jupyter with the outputs of your executed cells\n",
    "#\n",
    "# PS: Save again the notebook with this outcome.\n",
    "# PSPS: Don't forget to include the dataset in your submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Team:**\n",
    "* Abahana Zelalem\n",
    "* Benjamin Arnosti\n",
    "* Brandon Botezr\n",
    "\n",
    "**Course:** AI 574 – Natural Language Processing (Fall, 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "The objective of this project is to develop an effective SMS phishing detection system by leveraging cutting-edge NLP models such as BERT, RoBERTa, and GPT-3. These transformer-based models have revolutionized language understanding and exhibit the capability to detect phishing attempts in a more context-aware manner. By comparing their performance to traditional machine learning models like Logistic Regression, Support Vector Machines (SVM), and Long Short-Term Memory (LSTM) networks, this project aims to highlight the strengths and limitations of each approach. \n",
    "\n",
    "\n",
    "Our system will analyze both the textual content of SMS messages and any embedded features, such as URLs, email addresses, or phone numbers, to classify messages as phishing, smishing, spam, or legitimate (ham). While traditional models rely on handcrafted features and are generally faster, they may lack the sophistication needed to handle the complexities of modern phishing techniques. Transformer-based models, on the other hand, offer enhanced performance by capturing nuanced patterns in short messages, which is crucial in detecting sophisticated phishing attacks hidden in mobile text formats.\n",
    "\n",
    "\n",
    "By comparing traditional and advanced models, the project will provide insights into which models strike the best balance between performance, interpretability, and computational efficiency for real-time phishing detection in mobile environments.\n",
    "\n",
    "\n",
    "* **Keywords:** SMS, Phisishing, Smishing, text, phone, Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "* Source (url): https://data.mendeley.com/datasets/f45bkkt8pr/1/\n",
    "* Short Description : The dataset is a set of labelled text messages that have been collected for SMS Phishing research. It has 5971 text messages labeled as Legitimate (Ham) or Spam or Smishing. It includes 489 spam messages, 638 smishing messages, and 4844 ham messages. (Mishra, S., Soni, D., 2022)\n",
    "\n",
    "* Keywords: SMS, Phisishing, Smishing, text, phone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages\n",
    "\n",
    "- pandas\n",
    "- numpy\n",
    "- sklearn\n",
    "- Torch/PyTorch\n",
    "- transformers by HuggingFace\n",
    "- datasets by HuggingFace\n",
    "- matplotlib.pyplot\n",
    "- imblearn\n",
    "\n",
    "* These libraries can be installed via a Conda installer to create a virtual environmnet.  For specific information on installing PyTorch, see: https://pytorch.org/get-started/locally/.  For information on HuggingFace transformers or datasets, see: https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code begins here\n",
    "import os\n",
    "\n",
    "# Basic imports\n",
    "import pandas as pd\n",
    "import arrow\n",
    "import numpy as np\n",
    "import sklearn.utils\n",
    "\n",
    "# PyTorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# May use transformers\n",
    "import transformers\n",
    "# Get BERT from HF as well as Trainer class\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# Sklearn Metric items and splitting\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'You are usisng {device} on a {torch.cuda.get_device_name()}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Tokenizer Loading\n",
    "\n",
    "Models and tokenizers will be loaded here so they can be used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Model?\n",
    "# Should these go here or should they have their own notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Model Loads\n",
    "# Load the BERT-base-cased model\n",
    "BERT_model_name = 'google-bert/bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_model_name) # Keep on CPU\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_model_name, num_labels=3).to(device) # Move to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We are using the (Mishra & Soni, 2022) dataset for the project.  This dataset contains a collection of labeled SMS messages with labels indicating whether they are smishing or legitimate.  While the dataset is largely clean, we have had to do some pre-processing to match ‘Spam’ and ‘spam’ as well as ‘Ham’ and ‘ham’ labels.  Each entry includes the label (ham, spam, smishing), the SMS message, and if any URL, email address, or phone number is present.  Table 1 shows a sample of the dataset.  The dataset itself contains 5971 text messages of which 4844 are ham, 489 are spam, and 638 are smishing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from local directory\n",
    "ds = pd.read_csv(\"../data/processed/Dataset_5971.csv\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe I need to rename `label` to `labels`... I'll do this later because I need to do some clever work in getting labels to be integers for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = {'LABEL':'labels'}\n",
    "ds.rename(columns=cols, inplace=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels Spam and spam are the same as well as Smishing and smishing.  I'll push them all to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['labels'] = ds['labels'].str.lower()\n",
    "ds.labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A distribution of the sentence labels\n",
    "# This corresponds to the eventual binning of the 5 categories\n",
    "plt.hist(ds['labels'], bins=3)\n",
    "plt.title('Training Data Labels');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen by the histogram above the the data is heavily weighted to the ham values.  We may need to balance this dataset.\n",
    "\n",
    "I'm also curious just to see what the largest length of our data is.  BERT will only take a 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for item in ds['TEXT']:\n",
    "    if len(item) > max_len:\n",
    "        max_len = len(item)\n",
    "\n",
    "print(f'The max text length is: {max_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is longer than BERT's max input of 512.  I wonder what the average length is for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = 0\n",
    "for item in ds['TEXT']:\n",
    "    storage += len(item)\n",
    "\n",
    "print(f'The average text length is: {storage/len(ds):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot better for BERT.  What's the distribution of lengths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_list = []\n",
    "for item in ds['TEXT']:\n",
    "    length_list.append(len(item))\n",
    "\n",
    "plt.hist(length_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lengths are primarily smaller lengths.  There will be a lot of Padding then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values to categories.\n",
    "ds[['labels', 'URL', 'EMAIL', 'PHONE']] = ds[['labels', 'URL', 'EMAIL', 'PHONE']].astype('category')\n",
    "ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set category values to numeric values via codes\n",
    "ds['labels'] = ds['labels'].cat.codes\n",
    "ds['URL'] = ds['URL'].cat.codes\n",
    "ds['EMAIL'] = ds['EMAIL'].cat.codes\n",
    "ds['PHONE'] = ds['PHONE'].cat.codes\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets.\n",
    "train_ds, temp_ds = train_test_split(ds[['labels','TEXT', 'URL', 'EMAIL', 'PHONE']], test_size=0.2, random_state=226)\n",
    "val_ds, test_ds = train_test_split(temp_ds, test_size=0.3, random_state=226)\n",
    "\n",
    "\n",
    "train_ds = train_ds.reset_index(drop=True)\n",
    "val_ds = val_ds.reset_index(drop=True)\n",
    "test_ds = test_ds.reset_index(drop=True)\n",
    "\n",
    "print(f'Lengths of training: {len(train_ds)}')\n",
    "print(f'Lengths of validation: {len(val_ds)}')\n",
    "print(f'Lengths of test: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take another look at the dataset.\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the distributions between the train and test set look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training distribution between ham, spam, smish\n",
    "plt.hist(train_ds['labels']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val distribution between ham, spam, smish\n",
    "plt.hist(val_ds['labels']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test distribution between ham, spam, smish\n",
    "plt.hist(test_ds['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset # bring this in again to make sure I have the right one.\n",
    "\n",
    "# Move the train, val, and test into datasets to then be moved to a DatasetDict\n",
    "train_dataset = Dataset.from_pandas(train_ds)\n",
    "val_datset = Dataset.from_pandas(val_ds)\n",
    "test_dataset = Dataset.from_pandas(test_ds)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data to the DatasetDict\n",
    "data = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'val': val_datset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches the above histogram.\n",
    "\n",
    "We now have all the sentence data mapped to five categorical labels.  We are now in a good place to begin tokenizing and fine-tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Data\n",
    "\n",
    "We can now tokenize all of the sentences by fine-tuning the BERT tokenizer.\n",
    "Let's just run a quick tokenizer test to ensure we know how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick test to ensure the tokenizer is working as expected.\n",
    "test = tokenizer('Hello there.')\n",
    "print(f\"Input_ids: {test['input_ids']}\\nConversion back: {tokenizer.convert_ids_to_tokens(test['input_ids'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Length of the tokenizer: {len(tokenizer)}\\nCurrent word_embedding: {model.bert.embeddings.word_embeddings}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's tokenize everything with a function to use with map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizing function to apply via a map\n",
    "def tokenize_the_data(dskey):\n",
    "    # Set the max length to 512 as that is the BERT max.\n",
    "    tokenized_data = tokenizer(dskey['TEXT'], padding='max_length', max_length=512, truncation=True,\n",
    "                               return_tensors='pt', return_attention_mask=True )\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "tokenized_dataset = DatasetDict({\n",
    "    'train': train_dataset.map(tokenize_the_data),\n",
    "    'val': val_datset.map(tokenize_the_data),\n",
    "    'test': test_dataset.map(tokenize_the_data)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the tokenized_dataset for its layout\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some tokenized dataset values to ensure it worked as intended.\n",
    "text_holder = np.array(tokenized_dataset['train']['input_ids'][0]).flatten()\n",
    "# text_holder = text_holder[0]\n",
    "print(f\"Input_ids: {tokenized_dataset['train']['input_ids'][0]}\\nConversion back: {tokenizer.convert_ids_to_tokens(text_holder)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup and Formatting for Torch\n",
    "\n",
    "The tokenized dataset still has extra columns that are no longer needed.  While they should be automatically removed for the forward pass in the Trainer, I'll remove them manually here and save the Trainer the effort.\n",
    "\n",
    "I'll also end up running into an issue if I don't `squeeze()` my data as the trainer is looking for shapes of (batch, seq_len).  Right now there is an extra dimension that needs to be taken out which I'll do here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the dataset\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the unused features and reduce the dataset\n",
    "tokenized_dataset_reduced = tokenized_dataset.remove_columns(['TEXT', 'URL', 'EMAIL', 'PHONE', 'token_type_ids'])\n",
    "tokenized_dataset_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does everything exist that should exist.\n",
    "# I did drop token_type_ids which may be able to be passed along later... I'll have to get this working first.\n",
    "assert 'input_ids' in tokenized_dataset_reduced['train'].column_names\n",
    "assert 'attention_mask' in tokenized_dataset_reduced['train'].column_names\n",
    "assert 'labels' in tokenized_dataset_reduced['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_reduced.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a shaping issue that must be resolved for training.  this is because the tokenized_dataset_reduced['train']['input_ids'] is a list, rather than a Tensor.  I'll convert things here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the poor shaping\n",
    "print(f'Poor shape of input_ids:      {tokenized_dataset_reduced['train'][0]['input_ids'].shape}')\n",
    "print(f'Poor shape of attention_mask: {tokenized_dataset_reduced['train'][0]['attention_mask'].shape}')\n",
    "print(f'Poor shape of labels:         {tokenized_dataset_reduced['train'][0]['labels'].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciton to squeeze my dimensions\n",
    "def squeeze_dims(dskey):\n",
    "    if 'input_ids' in dskey:\n",
    "        dskey['input_ids'] = torch.squeeze(dskey['input_ids'])\n",
    "    if 'attention_mask' in dskey:\n",
    "        dskey['attention_mask'] = torch.squeeze(dskey['attention_mask'])\n",
    "    if 'labels' in dskey:\n",
    "        dskey['labels'] = torch.squeeze(dskey['labels'])\n",
    "\n",
    "    return dskey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze the dimensions.\n",
    "tokenized_dataset_reduced = tokenized_dataset_reduced.map(squeeze_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the good shaping\n",
    "print(f'Good shape of input_ids:      {tokenized_dataset_reduced['train'][0]['input_ids'].shape}')\n",
    "print(f'Good shape of attention_mask: {tokenized_dataset_reduced['train'][0]['attention_mask'].shape}')\n",
    "print(f'Good shape of labels:         {tokenized_dataset_reduced['train'][0]['labels'].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few labels and their types to double check they are integers for the categorization\n",
    "\n",
    "print(tokenized_dataset_reduced['train']['labels'][:10])  # First 10 labels\n",
    "print(type(tokenized_dataset_reduced['train']['labels'][0]))  # Type of the first label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a working dataset to use with the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up an Undersampled Dataset\n",
    "\n",
    "There is the change that given the heavy distribution of the data to ham values that there could be issues in the training of the models.  Here we'll set up a second dataset which is undersampled from the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling setups\n",
    "\n",
    "from imblearn import under_sampling\n",
    "from datasets import Dataset # Make sure I have the right Dataset\n",
    "# Resample here\n",
    "\n",
    "# Resample via undersampling up here\n",
    "\n",
    "# I'll cut everything down to the m items which is just below the lowest class and still easily batchable\n",
    "rus = under_sampling.RandomUnderSampler(random_state=226, replacement=False)\n",
    "\n",
    "# Create a copy of the dataset and convert it to DataFrame\n",
    "data_under = data.copy()\n",
    "train_df = pd.DataFrame(data_under['train'])\n",
    "\n",
    "# Ensure labels are in integer format\n",
    "train_df['labels'] = train_df['labels'].astype(int)\n",
    "\n",
    "# Separate features and labels\n",
    "X = train_df.drop(columns=['labels'])\n",
    "y = train_df['labels']\n",
    "\n",
    "# Apply RandomUnderSampler\n",
    "X_under, y_under = rus.fit_resample(X, y)\n",
    "\n",
    "# Combine the resampled features and labels back into a DataFrame\n",
    "train_resampled = X_under.copy()\n",
    "train_resampled['labels'] = y_under\n",
    "\n",
    "# Assign the resampled data back to the dataset\n",
    "data_usample = data.copy()\n",
    "data_usample['train'] = Dataset.from_pandas(train_resampled, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the new DatasetDict\n",
    "data_usample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the undersampling is even\n",
    "plt.hist(data_usample['train']['labels']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must go through all the data cleanup and formatting again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the data for training with the undersampled set\n",
    "\n",
    "# Tokenize the undersampled\n",
    "tokenized_usample_dataset = DatasetDict({\n",
    "    'train': data_usample['train'].map(tokenize_the_data),\n",
    "    'validation': data_usample['val'].map(tokenize_the_data),\n",
    "    'test': data_usample['test'].map(tokenize_the_data)\n",
    "})\n",
    "\n",
    "# Remove extra columns\n",
    "tokenized_usample_dataset_reduced = tokenized_dataset.remove_columns(['TEXT', 'URL', 'EMAIL', 'PHONE', 'token_type_ids'])\n",
    "tokenized_usample_dataset_reduced['train'] = tokenized_usample_dataset_reduced['train'].remove_columns([])\n",
    "tokenized_usample_dataset_reduced\n",
    "\n",
    "# Does everything exist that should exist.\n",
    "# I did drop token_type_ids which may be able to be passed along later... I'll have to get this working first.\n",
    "assert 'input_ids' in tokenized_usample_dataset_reduced['train'].column_names\n",
    "assert 'attention_mask' in tokenized_usample_dataset_reduced['train'].column_names\n",
    "assert 'labels' in tokenized_usample_dataset_reduced['train'].column_names\n",
    "\n",
    "# Set the Tensor format\n",
    "tokenized_usample_dataset_reduced.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Showing the poor shaping\n",
    "print(f'Poor shape of input_ids:      {tokenized_usample_dataset_reduced['train'][0]['input_ids'].shape}')\n",
    "print(f'Poor shape of attention_mask: {tokenized_usample_dataset_reduced['train'][0]['attention_mask'].shape}')\n",
    "print(f'Poor shape of labels:         {tokenized_usample_dataset_reduced['train'][0]['labels'].shape}')\n",
    "\n",
    "tokenized_usample_dataset_reduced = tokenized_usample_dataset_reduced.map(squeeze_dims)\n",
    "\n",
    "print(f'Good shape of input_ids:      {tokenized_usample_dataset_reduced['train'][0]['input_ids'].shape}')\n",
    "print(f'Good shape of attention_mask: {tokenized_usample_dataset_reduced['train'][0]['attention_mask'].shape}')\n",
    "print(f'Good shape of labels:         {tokenized_usample_dataset_reduced['train'][0]['labels'].shape}\\n')\n",
    "\n",
    "# Check the labels\n",
    "# Check the first few labels and their types to double check they are integers for the categorization\n",
    "print(tokenized_usample_dataset_reduced['train']['labels'][:10])  # First 10 labels\n",
    "print(type(tokenized_usample_dataset_reduced['train']['labels'][0]))  # Type of the first label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an undersampled dataset to use as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "1. Explan your Deep Learning process / methodology\n",
    "\n",
    "Our approach to classify ham, spam, and smishing messages using neural networks involves the following stages:\n",
    "1.\tText Preprocessing: The messages will undergo tokenization, stopword removal, and lemmatization. Additionally, URL features will be extracted and processed separately.\n",
    "\n",
    "2.\tUsing Traditional Models:\n",
    "o\tWe will use TensorFlow to build out traditional models such Logistic Regression and SVM, LSTM.  These baseline models will be used to classify messages based on TF-IDF features.\n",
    "\n",
    "3.\tAdvanced Models:\n",
    "- BERT: We will leverage a fine-tuned BERT model for SMS classification, leveraging its ability to capture contextual relationships even in short texts.  This BERT model uses a specific tokenizer on the full, unedited dataset.\n",
    "- `RoBERTa: As a more robust version of BERT, RoBERTa will be used for text classification, aiming to improve the precision of phishing detection.` - **REMOVE THIS??**\n",
    "- LLMs accessed via API: We will experiment with an LLM accessed via API for message generation to simulate phishing SMS scenarios and evaluate how well it detects sophisticated phishing attacks.  Some models we plan on accessing are Google’s Gemini and OpenAI’s Chat GPT-4.\n",
    "`4.\tEnsemble Method: We will explore ensemble learning to combine the predictions from different models, including both textual and URL-based classifiers.` - **REMOVE THIS??**\n",
    "\n",
    "\n",
    "2. Introduce the Deep Neural Networks you used in your project\n",
    " * RNN - [type?]\n",
    "    * Description \n",
    " \n",
    " * BERT\n",
    "    * BERT is a transformer model that learns bidirectional representations by predicting masked words and sentence relationships, enabling strong performance across various NLP tasks.\n",
    " \n",
    " * LLM\n",
    "     * Description \n",
    " \n",
    " \n",
    "**Keywords:** natural language processing, recurrent neural netowrks, transformers, sentiment analysis, multi-label classification, prediction, large language models\n",
    "\n",
    "___\n",
    "\n",
    "**Example**\n",
    "* ConvNet\n",
    "    * A convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery(source Wikipedia). \n",
    " \n",
    "* **Keywords:** supervised learning, classification, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting and Validation\n",
    "\n",
    "1. model 1 \n",
    "    - decription \n",
    "2. model 2\n",
    "    - decription "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code begins here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "* Examine your models (coefficients, parameters, errors, etc...)\n",
    "\n",
    "* Compute and interpret your results in terms of accuracy, precision, recall, ROC etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code begins here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT - Full Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tokenized data, let us fine-tune the BERT model.\n",
    "\n",
    "This is done with the Trainer class which takes TrainingArguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Training Args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10, # Prior runs show that I may be over fitting the data at 25 epochs...\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    # logging_steps=10, # This made my loss vs epoch plot too noisy...\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    ")\n",
    "\n",
    "# Set up the Trainer\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, return_tensors='pt')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_reduced['train'],\n",
    "    eval_dataset=tokenized_dataset_reduced['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Trainer\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "* Examine your models (coefficients, parameters, errors, etc...)\n",
    "\n",
    "* Compute and interpret your results in terms of accuracy, precision, recall, ROC etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the training and validation losses\n",
    "# Grab the losses from the trainer's log history.\n",
    "training_losses = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n",
    "evaluation_losses = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log and 'epoch' in log]\n",
    "\n",
    "# Plot the losses per epoch.\n",
    "plt.plot(range(0,len(training_losses)), training_losses, label='train')\n",
    "plt.plot(range(0, len(training_losses)), evaluation_losses, label='eval')\n",
    "plt.legend()\n",
    "plt.title('BERT Training and Validation Loss per epoch on Dataset');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At late epochs, we begin to see some signs of overfitting but they are not drastic yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from the test set\n",
    "preds = trainer.predict(tokenized_dataset_reduced['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the logits\n",
    "preds_logits = preds.predictions\n",
    "print(f'Example logit: {preds_logits[0]}')\n",
    "\n",
    "# Which label does the logit correspond to, use proper axis\n",
    "preds_labels = np.argmax(preds_logits, axis=1)\n",
    "print(f'Class Label: {preds_labels[0]}')\n",
    "\n",
    "# Get the actual labels from the test set\n",
    "true_labels = preds.label_ids\n",
    "print(f'True Label: {true_labels[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some metrics on the classification of the model\n",
    "class_report = metrics.classification_report(true_labels, preds_labels,\n",
    "                                             target_names=['C0', 'C1', 'C2'])\n",
    "\n",
    "print(f\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix to see where things are going sideways\n",
    "cm = metrics.confusion_matrix(true_labels, preds_labels)\n",
    "disp = metrics.ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix for BERT Dataset\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the predictions as well\n",
    "# Overlay the histograms\n",
    "\n",
    "plt.hist(preds_labels, bins=3, color='blue', alpha=0.5, label='Predictions')\n",
    "plt.hist(true_labels, bins=3, color='orange', alpha=0.5, label='Truth')\n",
    "plt.legend()\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Prediction and Truth Histograms from BERT on Dataset')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC CURVE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT - Undersampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build another BERT model for training\n",
    "model_usamp = BertForSequenceClassification.from_pretrained(BERT_model_name, num_labels=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Training Args\n",
    "training_args_usamp = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10, # Prior runs show that I may be over fitting the data at 25 epochs...\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    # logging_steps=10, # This made my loss vs epoch plot too noisy...\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    ")\n",
    "\n",
    "# Setup trainer\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, return_tensors='pt')\n",
    "\n",
    "trainer_usamp = Trainer(\n",
    "    model=model_usamp,\n",
    "    args=training_args_usamp,\n",
    "    train_dataset=tokenized_dataset_reduced['train'],\n",
    "    eval_dataset=tokenized_dataset_reduced['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the trainer\n",
    "trainer_usamp.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the training and validation losses\n",
    "# Grab the losses from the trainer's log history.\n",
    "training_losses = [log['loss'] for log in trainer_usamp.state.log_history if 'loss' in log]\n",
    "evaluation_losses = [log['eval_loss'] for log in trainer_usamp.state.log_history if 'eval_loss' in log and 'epoch' in log]\n",
    "\n",
    "# Plot the losses per epoch.\n",
    "plt.plot(range(0,len(training_losses)), training_losses, label='train')\n",
    "plt.plot(range(0, len(training_losses)), evaluation_losses, label='eval')\n",
    "plt.legend()\n",
    "plt.title('BERT Training and Validation Loss per epoch on Undersampled Dataset');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from the full test set\n",
    "preds = trainer_usamp.predict(tokenized_dataset_reduced['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the logits\n",
    "preds_logits = preds.predictions\n",
    "print(f'Example logit: {preds_logits[0]}')\n",
    "\n",
    "# Which label does the logit correspond to, use proper axis\n",
    "preds_labels = np.argmax(preds_logits, axis=1)\n",
    "print(f'Class Label: {preds_labels[0]}')\n",
    "\n",
    "# Get the actual labels from the test set\n",
    "true_labels = preds.label_ids\n",
    "print(f'True Label: {true_labels[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some metrics on the classification of the model\n",
    "class_report = metrics.classification_report(true_labels, preds_labels,\n",
    "                                             target_names=['C0', 'C1', 'C2'])\n",
    "\n",
    "print(f\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix to see where things are going sideways\n",
    "cm = metrics.confusion_matrix(true_labels, preds_labels)\n",
    "disp = metrics.ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix for BERT Dataset\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the predictions as well\n",
    "# Overlay the histograms\n",
    "\n",
    "plt.hist(preds_labels, bins=3, color='blue', alpha=0.5, label='Predictions')\n",
    "plt.hist(true_labels, bins=3, color='orange', alpha=0.5, label='Truth')\n",
    "plt.legend()\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Prediction and Truth Histograms from BERT on Dataset')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC CURVE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code begins here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation \n",
    "\n",
    "* Examine your models (coefficients, parameters, errors, etc...)\n",
    "\n",
    "* Compute and interpret your results in terms of accuracy, precision, recall, ROC etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code begins here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues / Improvements\n",
    "1. Dataset is very small\n",
    "2. Use regularization / initialization\n",
    "3. Use cross-validaiton\n",
    "4. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  References\n",
    "   - Academic (if any)\n",
    "   - Online (if any)\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits\n",
    "\n",
    "- If you use and/or adapt your code from existing projects, you must provide links and acknowldge the authors. Keep in mind that all documents in your projects and code will be check against the official plagiarism detection tool used by Penn State ([Turnitin](https://turnitin.psu.edu))\n",
    "\n",
    "> *This code is based on .... (if any)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of Project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai574_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
